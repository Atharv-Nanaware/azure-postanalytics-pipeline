{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3827a05-d632-42f2-bcf6-72eb2daff939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loding All External Loactions:\n",
    "\n",
    "checkpoint = spark.sql(\"describe external location `dev-checkpoints`\").select(\"url\").collect()[0][0]\n",
    "landing = spark.sql(\"describe external location `dev-landing`\").select(\"url\").collect()[0][0]\n",
    "bronze = spark.sql(\"describe external location `dev-bronze`\").select(\"url\").collect()[0][0]\n",
    "silver = spark.sql(\"describe external location `dev-silver`\").select(\"url\").collect()[0][0]\n",
    "gold = spark.sql(\"describe external location `dev-gold`\").select(\"url\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdfded21-70ab-4d7f-87f1-608749b19ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the environment variable\n",
    "\n",
    "dbutils.widgets.text(name=\"env\", defaultValue='', label='Enter the environment in lower case')\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8904d697-f8ea-4670-a87a-ffc55de095ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Unity Catalog and schemas dynamically\n",
    "catalog_name = f\"{env}_catalog\"\n",
    "bronze_schema = \"bronze\"\n",
    "silver_schema = \"silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ea4fbd-775c-4b6a-908c-11119560fe84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def normalize_column_names(df):\n",
    "    \n",
    "    for old_col in df.columns:\n",
    "        new_col = old_col.replace(\" \", \"_\")\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    print(\" Column names normalized!\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796522a4-a07c-4959-8cab-097731bb8f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp, date_format, col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def format_created_utc(df):\n",
    "    \"\"\"Ensures `created_utc` is TIMESTAMP and formats it as 'yyyy-MM-dd'.\"\"\"\n",
    "    df = df.withColumn(\"created_utc\", col(\"created_utc\").cast(TimestampType()))  \n",
    "    df = df.withColumn(\"created_utc\", date_format(from_utc_timestamp(col(\"created_utc\"), \"UTC\"), \"yyyy-MM-dd\"))\n",
    "    print(\" Formatted `created_utc` to 'yyyy-MM-dd'!\")\n",
    "    return df\n",
    "\n",
    "print(\"Function to format `created_utc` column loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83d7f90-eb4a-4092-a1dd-3bcc09569053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "def standardize_author(df):\n",
    "    \"\"\"Trims leading and trailing spaces from `author` column.\"\"\"\n",
    "    df = df.withColumn(\"author\", trim(col(\"author\")))\n",
    "    print(\" Standardized `author` column!\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d7979a-6d04-43a8-a191-1a39ebe4eba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_invalid_records(df):\n",
    "    \n",
    "    df = df.filter(\n",
    "        (col(\"id\").isNotNull()) & (col(\"id\") != \"\") &\n",
    "        (col(\"subreddit\").isNotNull()) & (col(\"subreddit\") != \"\") &\n",
    "        (col(\"title\").isNotNull()) & (col(\"title\") != \"\")\n",
    "    )\n",
    "    print(\"Removed invalid records!\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da202c3-a123-4b03-ab9a-791214ff5bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate_numerical_fields(df):\n",
    "    \n",
    "    df = df.filter(\n",
    "        (col(\"score\") >= 0) &\n",
    "        (col(\"num_comments\") >= 0) &\n",
    "        (col(\"upvote_ratio\").between(0.0, 1.0))\n",
    "    )\n",
    "    print(\"Validated numerical fields!\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5345a933-ba38-4a57-9bb7-33bb9cf471d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "def add_load_date(df):\n",
    "    \"\"\"Adds `load_date` column to track data ingestion.\"\"\"\n",
    "    df = df.withColumn(\"load_date\", current_date())\n",
    "    print(\" Added `load_date` column!\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c3fe4e6-8fdc-4813-add1-3895ad4b8eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_duplicates(df):\n",
    "    \n",
    "    df = df.dropDuplicates([\"id\"])\n",
    "    print(\" Removed duplicate records!\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9748b9e-2830-4fa2-a02e-b7f2a09b2d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "230713bf-87ee-4f75-bff2-1f1fb290ec5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as F_sum\n",
    "\n",
    "def data_quality_checks(df):\n",
    "   \n",
    "    print(\"Running Data Quality Checks...\")\n",
    "\n",
    "    # Count NULLs in important columns \n",
    "    null_counts = df.select([\n",
    "        F_sum(col(c).isNull().cast(\"int\")).alias(c) for c in [\"id\", \"subreddit\", \"title\", \"created_utc\"]\n",
    "    ])\n",
    "    \n",
    "    print(\" NULL Counts:\")\n",
    "    null_counts.show()\n",
    "\n",
    "    # Check for duplicate `id`s\n",
    "    duplicate_count = df.groupBy(\"id\").count().filter(col(\"count\") > 1).count()\n",
    "    print(f\" Duplicate IDs: {duplicate_count}\")\n",
    "\n",
    "    # Check valid ranges for numerical columns\n",
    "    invalid_scores = df.filter(col(\"score\") < 0).count()\n",
    "    invalid_comments = df.filter(col(\"num_comments\") < 0).count()\n",
    "    invalid_upvote = df.filter(~col(\"upvote_ratio\").between(0.0, 1.0)).count()\n",
    "\n",
    "    print(f\" Invalid Scores: {invalid_scores}\")\n",
    "    print(f\" Invalid Num_Comments: {invalid_comments}\")\n",
    "    print(f\"Invalid Upvote Ratios: {invalid_upvote}\")\n",
    "\n",
    "    print(\"Data Quality Checks Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2bc3f9d-1061-4e13-9ad2-3fb599addf6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Applying Tranformations And Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c99158-1e1f-4c4a-bdbb-2ca44a5d7dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load Bronze Table\n",
    "\n",
    "df_bronze = spark.read.table(f\"`{catalog_name}`.`{bronze_schema}`.raw_redditPosts\")\n",
    "# df_bronze.display()\n",
    "\n",
    "# # Apply transformations one by one\n",
    "df_silver = normalize_column_names(df_bronze)\n",
    "\n",
    "# df_silver.select(\"created_utc\").show(5, truncate=False)\n",
    "\n",
    "df_silver = format_created_utc(df_silver)\n",
    "df_silver = standardize_author(df_silver)\n",
    "df_silver = filter_invalid_records(df_silver)\n",
    "df_silver = validate_numerical_fields(df_silver)\n",
    "df_silver = add_load_date(df_silver)\n",
    "df_silver = remove_duplicates(df_silver)\n",
    "\n",
    "\n",
    "# Run Data Quality Checks\n",
    "data_quality_checks(df_silver)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb8f22f-6483-4478-b9b3-962c089ceb47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.types import FloatType, StructType, StructField\n",
    "\n",
    "\n",
    "# Initialize VADER Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 2Define Sentiment Analysis Function\n",
    "def analyze_sentiment_vader(text):\n",
    "    \"\"\"Returns positive, neutral, negative, and compound sentiment scores using VADER.\"\"\"\n",
    "    if text is None or text.strip() == \"\":\n",
    "        return (0.0, 0.0, 0.0, 0.0)  \n",
    "    \n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return (scores[\"pos\"], scores[\"neu\"], scores[\"neg\"], scores[\"compound\"])\n",
    "\n",
    "# Define UDF for PySpark\n",
    "sentiment_udf = udf(analyze_sentiment_vader, StructType([\n",
    "    StructField(\"positive\", FloatType(), True),\n",
    "    StructField(\"neutral\", FloatType(), True),\n",
    "    StructField(\"negative\", FloatType(), True),\n",
    "    StructField(\"compound\", FloatType(), True)\n",
    "]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866e76e7-0e8b-4fba-9ff8-15c3af47e3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply VADER Sentiment Analysis\n",
    "df_silver = df_silver.withColumn(\"sentiment\", sentiment_udf(col(\"title\")))\n",
    "df_silver = df_silver.withColumn(\"positive_score\", col(\"sentiment\").positive) \\\n",
    "                     .withColumn(\"neutral_score\", col(\"sentiment\").neutral) \\\n",
    "                     .withColumn(\"negative_score\", col(\"sentiment\").negative) \\\n",
    "                     .withColumn(\"compound_score\", col(\"sentiment\").compound) \\\n",
    "                     .drop(\"sentiment\")\n",
    "\n",
    "print(\"Applied VADER Sentiment Analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c3d3f0-6269-47e4-a83c-2253d40a7400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Write to Silver Table\n",
    "df_silver.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "        f\"{catalog_name}.{silver_schema}.cleaned_redditPosts\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Silver processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cecec4d-52a4-4a23-bacd-03ef7301675e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validations\n",
    "df = spark.sql(f\"SELECT COUNT(*) FROM {env}_catalog.silver.cleaned_redditPosts\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf893b8-e68a-4122-b552-9f798e758a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"SELECT * FROM {env}_catalog.silver.cleaned_redditPosts\")\n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-SilverTranfomationsAndSentiment",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "4565fc64-1f7d-46d4-bba7-2b50e9d758d5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
